1.線性回歸演算法:
線性回歸是一種用於建模和分析變數之間關係的統計方法。它假設自變數（特徵）和因變數之間存在線性關係。這種方法被廣泛應用於機器學習和統計學領域，特別是在預測和回歸分析方面。
2.分類演算法:
分類演算法是機器學習中一個重要的主題，它的目標是從數據中學習一個模型，使其能夠將新的數據點分為不同的類別。以下是一些常見的分類演算法：
  2.1:決策樹（Decision Trees）：
    決策樹通過遞歸地將數據集分割成子集，形成一個樹狀結構。
    常見的分割標準有信息增益、GINI指數等。
    容易理解和可視化，但容易過擬合。
  2.2:隨機森林（Random Forest）：
    隨機森林是多個決策樹的集成學習模型。
    每個決策樹基於隨機選取的子集和特徵進行訓練。
    通常具有較好的性能和泛化能力。
  2.3:支持向量機（Support Vector Machines，SVM）：
    SVM通過在特徵空間中找到最佳的超平面，將數據集分為不同的類別。
    可以使用不同的內核函數應對非線性問題。
    對於高維數據和小樣本量的情況效果較好。
  2.4:K最近鄰算法（K-Nearest Neighbors，KNN）：
    KNN根據數據點之間的距離，將新的數據點分類到與其最近的K個鄰居中的多數類別。
    對於小型數據集和非常大的特徵集合適用。
  2.5:邏輯回歸（Logistic Regression）：
    邏輯回歸用於二元分類問題，通過應用邏輯函數（sigmoid函數）來估計概率。
    可以透過正則化處理多重共線性。
  2.6:人工神經網絡（Artificial Neural Networks，ANN）：
    ANN是受到生物神經網絡啟發的模型，包含多層神經元。
    深度神經網絡（Deep Neural Networks，DNN）是一種深度學習方法，用於處理複雜的非線性問題。
  2.7:梯度提升機（Gradient Boosting Machines）：
    梯度提升機是一種集成學習方法，通過迭代地訓練弱學習器來提升模型性能。
    常見的實現有梯度提升樹（Gradient Boosted Trees）。
  2.8:貝葉斯分類器（Naive Bayes Classifier）：
    貝葉斯分類器基於貝葉斯定理，假設特徵之間相互獨立。
    適用於文本分類和垃圾郵件檢測等應用。
3.基於核(kernel-based)的演算法:
基於核的演算法是一類利用核函數（kernel function）將原始特徵映射到高維空間的機器學習演算法。這種方法通常應用於支持向量機（Support Vector Machines，SVM）和核岭迴歸等模型，它們能夠處理非線性的數據集並更好地擬合複雜的模型。以下是一些基於核的演算法：
  3.1:支持向量機（Support Vector Machines，SVM）：
    SVM 是一種二元分類和回歸的機器學習模型。
    通過使用核函數將數據映射到高維空間，SVM可以處理非線性分類問題。
    常見的核函數包括線性核、多項式核和高斯核（RBF核）等。
  3.2:核岭迴歸（Kernel Ridge Regression）：
    核岭迴歸是線性岭迴歸的擴展，使用核技巧處理非線性迴歸問題。
    通過核函數將特徵映射到高維空間，然後在高維空間中擬合岭迴歸模型。
  3.3:核 PCA（Kernel Principal Component Analysis）：
    核主成分分析是主成分分析（PCA）的擴展，利用核函數處理非線性降維。
    通過映射到高維空間，然後找到新空間中的主成分。
  3.4:核 Fisher 判別分析（Kernel Fisher Discriminant Analysis）：
    核 Fisher 判別分析是 Fisher 判別分析的擴展，同樣使用核函數處理非線性分類問題。
    通過映射到高維空間，然後在新空間中找到最能區分不同類別的方向。
  3.5:核 k-均值（Kernel k-Means）：
    核 k-均值是k-均值聚類算法的擴展，使用核函數處理非線性分簇。
    通過將數據映射到高維空間，然後在新空間中執行k-均值算法。
這些基於核的演算法能夠處理非線性問題，並在高維空間中進行計算，但也需要謹慎選擇適當的核函數和超參數，以避免過擬合。







